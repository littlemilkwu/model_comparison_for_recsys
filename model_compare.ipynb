{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from surprise import accuracy, Dataset, Reader, KNNWithMeans, NMF\n",
    "from surprise.model_selection import cross_validate, train_test_split\n",
    "from collections import defaultdict\n",
    "from config import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_interaction(df_inter):\n",
    "    print('origin shape: ', df_inter.shape)\n",
    "    vc_user = df_inter['user_id'].value_counts()\n",
    "    mask = df_inter['user_id'].isin(vc_user[vc_user >= 3].index)\n",
    "    df_inter = df_inter.loc[mask].reset_index(drop=True)\n",
    "    print('> 3 interactions: ', df_inter.shape)\n",
    "    return df_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin shape:  (792062, 3)\n",
      "> 3 interactions:  (790197, 3)\n"
     ]
    }
   ],
   "source": [
    "# douban book\n",
    "df_douban_inter = pd.read_csv(data_path_douban + 'user_book.dat', sep='\\t', header=None).rename({\n",
    "    0: 'user_id',\n",
    "    1: 'item_id',\n",
    "    2: 'rating',\n",
    "}, axis=1)\n",
    "df_douban_inter['user_id'] = df_douban_inter['user_id'].astype('str')\n",
    "df_douban_inter['item_id'] = df_douban_inter['item_id'].astype('str')\n",
    "df_douban_inter = filter_interaction(df_douban_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin shape:  (100000, 4)\n",
      "> 3 interactions:  (100000, 4)\n"
     ]
    }
   ],
   "source": [
    "# movie lens\n",
    "df_movie_inter = pd.read_csv(data_path_movie + 'user_movie.dat', sep='\\t', header=None).rename({\n",
    "    0: 'user_id',\n",
    "    1: 'item_id',\n",
    "    2: 'rating',\n",
    "}, axis=1)\n",
    "df_movie_inter['user_id'] = df_movie_inter['user_id'].astype('str')\n",
    "df_movie_inter['item_id'] = df_movie_inter['item_id'].astype('str')\n",
    "df_movie_inter = filter_interaction(df_movie_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kf = KFold(n_splits=5, random_state=RANDOM_STATE, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering & MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cf_mf():\n",
    "    algo_UCF_s = KNNWithMeans(sim_options={\n",
    "        \"name\": \"cosine\",\n",
    "        \"user_based\": True\n",
    "    })\n",
    "    algo_UCF_p = KNNWithMeans(sim_options={\n",
    "        \"name\": \"pearson_baseline\",\n",
    "        \"user_based\": True,\n",
    "        'shrinkage': 0\n",
    "    })\n",
    "\n",
    "    algo_ICF_s = KNNWithMeans(sim_options={\n",
    "        \"name\": \"cosine\",\n",
    "        \"user_based\": False\n",
    "    })\n",
    "\n",
    "    algo_ICF_p = KNNWithMeans(sim_options={\n",
    "        \"name\": \"pearson_baseline\",\n",
    "        \"user_based\": False,\n",
    "        'shrinkage': 0\n",
    "    })\n",
    "    algo_NMF = NMF()\n",
    "    return algo_UCF_s, algo_UCF_p, algo_ICF_s, algo_ICF_p, algo_NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_eval_metrics(predictions, k=10, threshold=3.5):\n",
    "    rmse = accuracy.rmse(predictions)\n",
    "    ls_recall = []\n",
    "    ls_ndcg = []\n",
    "    \n",
    "    dict_user_ratings = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        dict_user_ratings[uid].append((est, true_r))\n",
    "\n",
    "    for uid, ratings in dict_user_ratings.items():\n",
    "        ratings.sort(lambda x: x[0], reverse=True)\n",
    "        true_ratings = sorted(ratings, key=lambda x: x[1], reverse=True)\n",
    "        # recall\n",
    "        num_rel = sum(1 for (est, true_r) in ratings if true_r > threshold)\n",
    "        num_rel_top_k = sum(1 for (est, true_r) in ratings[:k] if true_r > threshold and est > threshold)\n",
    "        recall = num_rel_top_k / num_rel if num_rel != 0 else 0\n",
    "        ls_recall.append(recall)\n",
    "\n",
    "        # ndcg\n",
    "        dcg = sum(true_r / math.log(i + 2, base=2) for i, (est, true_r) in enumerate(ratings[:k]))\n",
    "        idcg = sum(true_r / math.log(i + 2, base=2) for i, (est, true_r) in enumerate(true_ratings[:k]))\n",
    "        ndcg = dcg / idcg\n",
    "        ls_ndcg.append(ndcg)\n",
    "\n",
    "    mean_recall = sum(ls_recall) / len(ls_recall)\n",
    "    mean_ndcg = sum(ls_ndcg) / len(ls_ndcg)\n",
    "    \n",
    "    return [rmse, mean_recall, mean_ndcg]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## douban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Fold 0 *****\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n"
     ]
    }
   ],
   "source": [
    "# 1 fold => 3m 13.4s\n",
    "\n",
    "ls_result = []\n",
    "for i, (train_idx, test_idx) in enumerate(Kf.split(df_douban_inter)):\n",
    "    print(f'\\n***** Fold {i} *****')\n",
    "    algo_UCF_s, algo_UCF_p, algo_ICF_s, algo_ICF_p, algo_NMF = init_cf_mf()\n",
    "    \n",
    "    df_train = df_douban_inter.iloc[train_idx]\n",
    "    df_test = df_douban_inter.iloc[test_idx]\n",
    "\n",
    "    reader = Reader(rating_scale=(1, 5))\n",
    "    train_set = Dataset.load_from_df(df_train[['user_id', 'item_id', 'rating']], reader=reader)\n",
    "    train_set = train_set.build_full_trainset()\n",
    "\n",
    "    algo_UCF_s.fit(train_set)\n",
    "    algo_UCF_p.fit(train_set)\n",
    "    algo_ICF_s.fit(train_set)\n",
    "    algo_ICF_p.fit(train_set)\n",
    "    algo_NMF.fit(train_set)\n",
    "\n",
    "    pred_UCF_s = algo_UCF_s.test(df_test.values)\n",
    "    pred_UCF_p = algo_UCF_p.test(df_test.values)\n",
    "    pred_ICF_s = algo_ICF_s.test(df_test.values)\n",
    "    pred_ICF_p = algo_ICF_p.test(df_test.values)\n",
    "    pred_NMF = algo_NMF.test(df_test.values)\n",
    "\n",
    "    metrics_UCF_s = ['UCF_s', i] + calc_eval_metrics(pred_UCF_s)\n",
    "    metrics_UCF_p = ['UCF_p', i] + calc_eval_metrics(pred_UCF_p)\n",
    "    metrics_ICF_s = ['ICF_s', i] + calc_eval_metrics(pred_ICF_s)\n",
    "    metrics_ICF_p = ['ICF_p', i] + calc_eval_metrics(pred_ICF_p)\n",
    "    metrics_NMF = ['NMF', i] + calc_eval_metrics(pred_NMF)\n",
    "\n",
    "    ls_result.extend([metrics_UCF_s, metrics_UCF_p, metrics_ICF_s, metrics_ICF_p, metrics_NMF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterhub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
