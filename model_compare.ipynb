{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import KFold\n",
    "from surprise import accuracy, Dataset, Reader, KNNWithMeans, NMF\n",
    "from surprise.model_selection import cross_validate, train_test_split\n",
    "from collections import defaultdict\n",
    "from config import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kf = KFold(n_splits=5, random_state=RANDOM_STATE, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_douban_inter = pd.read_csv(data_preprocessing + 'douban_inter.csv', dtype={'user_id': str, 'item_id': str,})\n",
    "df_movie_inter = pd.read_csv(data_preprocessing + 'movie_inter.csv', dtype={'user_id': str, 'item_id': str,})\n",
    "df_yelp_inter = pd.read_csv(data_preprocessing + 'yelp_inter.csv', dtype={'user_id': str, 'item_id': str,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_douban_user_features = pd.read_csv(data_preprocessing + 'douban_user_features.csv', dtype=str)\n",
    "df_douban_user_features['location'] = 'location:' + df_douban_user_features['location']\n",
    "\n",
    "df_douban_item_features = pd.read_csv(data_preprocessing + 'douban_item_features.csv', dtype=str)\n",
    "for col in df_douban_item_features.columns[1:]:\n",
    "    df_douban_item_features[col] = col + \":\" + df_douban_item_features[col]\n",
    "# df_douban_item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movie_user_features = pd.read_csv(data_preprocessing + \"movie_user_features.csv\", dtype=str)\n",
    "for col in df_movie_user_features.columns[1:]:\n",
    "    df_movie_user_features[col] = col + \":\" + df_movie_user_features[col]\n",
    "# df_movie_user_features\n",
    "\n",
    "df_movie_item_features = pd.read_csv(data_preprocessing + 'movie_item_features.csv', dtype=str)\n",
    "for col in df_movie_item_features.columns[1:]:\n",
    "    df_movie_item_features[col] = col + \":\" + df_movie_item_features[col]\n",
    "# df_movie_item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp_user_features = pd.read_csv(data_preprocessing + \"yelp_user_features.csv\", dtype=str)\n",
    "for col in df_yelp_user_features.columns[1:]:\n",
    "    df_yelp_user_features[col] = col + \":\" + df_yelp_user_features[col]\n",
    "# df_yelp_user_features\n",
    "\n",
    "df_yelp_item_features = pd.read_csv(data_preprocessing + \"yelp_item_features.csv\", dtype=str)\n",
    "for col in df_yelp_item_features.columns[1:]:\n",
    "    df_yelp_item_features[col] = col + \":\" + df_yelp_item_features[col]\n",
    "# df_yelp_item_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering & MF\n",
    "* [Surprise Basic Usage](https://surprise.readthedocs.io/en/stable/getting_started.html)\n",
    "* [Surprise Prediction Algorithms List](https://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html)\n",
    "* [Surprise Similarity Options](https://surprise.readthedocs.io/en/stable/prediction_algorithms.html)\n",
    "* [Surprise Accuracy RMSE](https://surprise.readthedocs.io/en/stable/accuracy.html)\n",
    "* [Surprise Recall](https://surprise.readthedocs.io/en/stable/FAQ.html)\n",
    "* [NDGC 計算方式](https://ithelp.ithome.com.tw/articles/10299050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cf_mf():\n",
    "    algo_UCF_s = KNNWithMeans(sim_options={\n",
    "        \"name\": \"cosine\",\n",
    "        \"user_based\": True\n",
    "    })\n",
    "    algo_UCF_p = KNNWithMeans(sim_options={\n",
    "        \"name\": \"pearson_baseline\",\n",
    "        \"user_based\": True,\n",
    "        'shrinkage': 0\n",
    "    })\n",
    "\n",
    "    algo_ICF_s = KNNWithMeans(sim_options={\n",
    "        \"name\": \"cosine\",\n",
    "        \"user_based\": False\n",
    "    })\n",
    "\n",
    "    algo_ICF_p = KNNWithMeans(sim_options={\n",
    "        \"name\": \"pearson_baseline\",\n",
    "        \"user_based\": False,\n",
    "        'shrinkage': 0\n",
    "    })\n",
    "    algo_NMF = NMF(n_factors=FACTORS, n_epochs=EPOCHS)\n",
    "    return algo_UCF_s, algo_UCF_p, algo_ICF_s, algo_ICF_p, algo_NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cf_eval_metrics(predictions, k=10, threshold=3.5):\n",
    "    rmse = accuracy.rmse(predictions)\n",
    "    ls_recall = []\n",
    "    ls_ndcg = []\n",
    "    \n",
    "    dict_user_ratings = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        dict_user_ratings[uid].append((est, true_r))\n",
    "\n",
    "    for uid, ratings in dict_user_ratings.items():\n",
    "        ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        true_ratings = sorted(ratings, key=lambda x: x[1], reverse=True)\n",
    "        # recall\n",
    "        num_rel = sum(1 for (est, true_r) in ratings if true_r > threshold)\n",
    "        num_rel_top_k = sum(1 for (est, true_r) in ratings[:k] if true_r > threshold and est > threshold)\n",
    "        recall = num_rel_top_k / num_rel if num_rel != 0 else 0\n",
    "        ls_recall.append(recall)\n",
    "\n",
    "        # ndcg\n",
    "        dcg = sum(true_r / math.log(i + 2, 2) for i, (est, true_r) in enumerate(ratings[:k]))\n",
    "        idcg = sum(true_r / math.log(i + 2, 2) for i, (est, true_r) in enumerate(true_ratings[:k]))\n",
    "        ndcg = dcg / idcg\n",
    "        ls_ndcg.append(ndcg)\n",
    "\n",
    "    mean_recall = sum(ls_recall) / len(ls_recall)\n",
    "    mean_ndcg = sum(ls_ndcg) / len(ls_ndcg)\n",
    "    \n",
    "    return [rmse, mean_recall, mean_ndcg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 fold => 3m 13.4s\n",
    "def calc_all_cf_model(df_inter, data_name):\n",
    "    ls_result = []\n",
    "    for i, (train_idx, test_idx) in enumerate(Kf.split(df_inter)):\n",
    "        print(f'\\n***** Fold {i} *****')\n",
    "        algo_UCF_s, algo_UCF_p, algo_ICF_s, algo_ICF_p, algo_NMF = init_cf_mf()\n",
    "        \n",
    "        df_train = df_inter.iloc[train_idx]\n",
    "        df_test = df_inter.iloc[test_idx]\n",
    "\n",
    "        reader = Reader(rating_scale=(1, 5))\n",
    "        train_set = Dataset.load_from_df(df_train[['user_id', 'item_id', 'rating']], reader=reader)\n",
    "        train_set = train_set.build_full_trainset()\n",
    "\n",
    "        algo_UCF_s.fit(train_set)\n",
    "        algo_UCF_p.fit(train_set)\n",
    "        algo_ICF_s.fit(train_set)\n",
    "        algo_ICF_p.fit(train_set)\n",
    "        algo_NMF.fit(train_set)\n",
    "\n",
    "        pred_UCF_s = algo_UCF_s.test(df_test.values)\n",
    "        pred_UCF_p = algo_UCF_p.test(df_test.values)\n",
    "        pred_ICF_s = algo_ICF_s.test(df_test.values)\n",
    "        pred_ICF_p = algo_ICF_p.test(df_test.values)\n",
    "        pred_NMF = algo_NMF.test(df_test.values)\n",
    "\n",
    "        metrics_UCF_s = ['UCF_s', i, data_name] + calc_cf_eval_metrics(pred_UCF_s)\n",
    "        metrics_UCF_p = ['UCF_p', i, data_name] + calc_cf_eval_metrics(pred_UCF_p)\n",
    "        metrics_ICF_s = ['ICF_s', i, data_name] + calc_cf_eval_metrics(pred_ICF_s)\n",
    "        metrics_ICF_p = ['ICF_p', i, data_name] + calc_cf_eval_metrics(pred_ICF_p)\n",
    "        metrics_NMF = ['NMF', i, data_name] + calc_cf_eval_metrics(pred_NMF)\n",
    "\n",
    "        ls_result.extend([metrics_UCF_s, metrics_UCF_p, metrics_ICF_s, metrics_ICF_p, metrics_NMF])\n",
    "    return ls_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_result = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### douban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Fold 0 *****\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.7171\n",
      "RMSE: 0.7271\n",
      "RMSE: 0.7102\n",
      "RMSE: 0.7227\n",
      "RMSE: 1.0990\n",
      "\n",
      "***** Fold 1 *****\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.7134\n",
      "RMSE: 0.7248\n",
      "RMSE: 0.7076\n",
      "RMSE: 0.7204\n",
      "RMSE: 1.0971\n",
      "\n",
      "***** Fold 2 *****\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.7130\n",
      "RMSE: 0.7244\n",
      "RMSE: 0.7064\n",
      "RMSE: 0.7199\n",
      "RMSE: 1.0931\n",
      "\n",
      "***** Fold 3 *****\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.7131\n",
      "RMSE: 0.7236\n",
      "RMSE: 0.7063\n",
      "RMSE: 0.7195\n",
      "RMSE: 1.0948\n",
      "\n",
      "***** Fold 4 *****\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.7101\n",
      "RMSE: 0.7221\n",
      "RMSE: 0.7042\n",
      "RMSE: 0.7172\n",
      "RMSE: 1.0962\n"
     ]
    }
   ],
   "source": [
    "ls_result = calc_all_cf_model(df_douban_inter, 'douban')\n",
    "all_result.extend(ls_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Fold 0 *****\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9587\n",
      "RMSE: 0.9543\n",
      "RMSE: 0.9469\n",
      "RMSE: 0.9440\n",
      "RMSE: 1.3074\n",
      "\n",
      "***** Fold 1 *****\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9554\n",
      "RMSE: 0.9488\n",
      "RMSE: 0.9441\n",
      "RMSE: 0.9427\n",
      "RMSE: 1.2997\n",
      "\n",
      "***** Fold 2 *****\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9537\n",
      "RMSE: 0.9500\n",
      "RMSE: 0.9386\n",
      "RMSE: 0.9369\n",
      "RMSE: 1.2951\n",
      "\n",
      "***** Fold 3 *****\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9547\n",
      "RMSE: 0.9490\n",
      "RMSE: 0.9445\n",
      "RMSE: 0.9421\n",
      "RMSE: 1.3009\n",
      "\n",
      "***** Fold 4 *****\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9593\n",
      "RMSE: 0.9550\n",
      "RMSE: 0.9472\n",
      "RMSE: 0.9446\n",
      "RMSE: 1.3012\n"
     ]
    }
   ],
   "source": [
    "ls_result = calc_all_cf_model(df_movie_inter, 'movie_len')\n",
    "all_result.extend(ls_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Fold 0 *****\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0823\n",
      "RMSE: 1.1398\n",
      "RMSE: 1.0850\n",
      "RMSE: 1.1391\n",
      "RMSE: 1.3201\n",
      "\n",
      "***** Fold 1 *****\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0746\n",
      "RMSE: 1.1275\n",
      "RMSE: 1.0826\n",
      "RMSE: 1.1390\n",
      "RMSE: 1.3182\n",
      "\n",
      "***** Fold 2 *****\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0872\n",
      "RMSE: 1.1412\n",
      "RMSE: 1.0858\n",
      "RMSE: 1.1396\n",
      "RMSE: 1.3202\n",
      "\n",
      "***** Fold 3 *****\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0821\n",
      "RMSE: 1.1309\n",
      "RMSE: 1.0842\n",
      "RMSE: 1.1378\n",
      "RMSE: 1.3200\n",
      "\n",
      "***** Fold 4 *****\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0851\n",
      "RMSE: 1.1363\n",
      "RMSE: 1.0844\n",
      "RMSE: 1.1380\n",
      "RMSE: 1.3253\n"
     ]
    }
   ],
   "source": [
    "ls_result = calc_all_cf_model(df_yelp_inter, 'yelp')\n",
    "all_result.extend(ls_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_result, columns=['model', 'kfold', 'data', 'RMSE', 'Recall', 'NDCG']).to_csv('output/CF_MF_result.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorization Machine & BPR\n",
    "* 會將數值型特徵直接全展開 ^_^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_fm_mf():\n",
    "    model_fm = LightFM(no_components=FACTORS)\n",
    "    model_fm_bpr = LightFM(no_components=FACTORS, loss='bpr')\n",
    "    model_mf_bpr = LightFM(no_components=FACTORS, loss='bpr')\n",
    "    return model_fm, model_fm_bpr, model_mf_bpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fm_evaluation(df_test, pred, k=10, threshold=3.5):\n",
    "    ls_recall = []\n",
    "    ls_ndcg = []\n",
    "    dict_user_ratings = defaultdict(list)\n",
    "\n",
    "    for i, x in df_test.iterrows():\n",
    "        dict_user_ratings[x['user_id']].append((pred[i], x['rating']))\n",
    "    \n",
    "    for uid, ratings in dict_user_ratings.items():\n",
    "        ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        true_ratings = sorted(ratings, key=lambda x: x[1], reverse=True)\n",
    "        # recall\n",
    "        num_rel = sum(1 for (est, true_r) in ratings if true_r > threshold)\n",
    "        num_rel_top_k = sum(1 for (est, true_r) in ratings[:k] if true_r > threshold and est > threshold)\n",
    "        recall = num_rel_top_k / num_rel if num_rel != 0 else 0\n",
    "        ls_recall.append(recall)\n",
    "\n",
    "        # ndcg\n",
    "        dcg = sum(true_r / math.log(i + 2, 2) for i, (est, true_r) in enumerate(ratings[:k]))\n",
    "        idcg = sum(true_r / math.log(i + 2, 2) for i, (est, true_r) in enumerate(true_ratings[:k]))\n",
    "        ndcg = dcg / idcg\n",
    "        ls_ndcg.append(ndcg)\n",
    "\n",
    "    mean_recall = sum(ls_recall) / len(ls_recall)\n",
    "    mean_ndcg = sum(ls_ndcg) / len(ls_ndcg)\n",
    "    \n",
    "    return [mean_recall, mean_ndcg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 fold => \n",
    "def calc_all_fm_model(df_inter, fm_data, user_features, item_features, data_name):\n",
    "    ls_result = []\n",
    "    for i, (train_idx, test_idx) in enumerate(Kf.split(df_inter)):\n",
    "        print(f'\\n***** Fold {i} *****')\n",
    "        model_fm, model_fm_bpr, model_mf_bpr = init_fm_mf()\n",
    "\n",
    "        df_train = df_inter.iloc[train_idx].reset_index(drop=True)\n",
    "        df_test = df_inter.iloc[test_idx].reset_index(drop=True)\n",
    "        map_user_id, _, map_item_id, _ = fm_data.mapping()\n",
    "        df_train = df_train.loc[df_train['rating'] > 3.5].reset_index(drop=True)\n",
    "        train_interactions, train_weights = fm_data.build_interactions(((x['user_id'], x['item_id']) for idx, x in df_train.iterrows()))\n",
    "\n",
    "        model_fm.fit(train_interactions,\n",
    "                     user_features=user_features, \n",
    "                     item_features=item_features, \n",
    "                     epochs=EPOCHS, num_threads=THREADS, verbose=True)\n",
    "        model_fm_bpr.fit(train_interactions,\n",
    "                         user_features=user_features,\n",
    "                         item_features=item_features,\n",
    "                         epochs=EPOCHS, num_threads=THREADS, verbose=True)\n",
    "        model_mf_bpr.fit(train_interactions, \n",
    "                         epochs=EPOCHS, num_threads=THREADS, verbose=True)\n",
    "        pred_fm = model_fm.predict(\n",
    "            [map_user_id[uid] for uid in df_test['user_id']],\n",
    "            [map_item_id[iid] for iid in df_test['item_id']],\n",
    "            user_features=user_features, item_features=item_features, \n",
    "            num_threads=THREADS)\n",
    "        \n",
    "        pred_fm_bpr = model_fm_bpr.predict(\n",
    "            [map_user_id[uid] for uid in df_test['user_id']],\n",
    "            [map_item_id[iid] for iid in df_test['item_id']],\n",
    "            user_features=user_features, item_features=item_features, \n",
    "            num_threads=THREADS)\n",
    "        \n",
    "        pred_mf_bpr = model_mf_bpr.predict(\n",
    "            [map_user_id[uid] for uid in df_test['user_id']],\n",
    "            [map_item_id[iid] for iid in df_test['item_id']],\n",
    "            num_threads=THREADS)\n",
    "\n",
    "        metrics_fm = ['fm', i, data_name] + calc_fm_evaluation(df_test, pred_fm)\n",
    "        metrics_fm_bpr = ['fm_bpr', i, data_name] + calc_fm_evaluation(df_test, pred_fm_bpr)\n",
    "        metrics_mf_bpr = ['mf_bpr', i, data_name] + calc_fm_evaluation(df_test, pred_mf_bpr)\n",
    "\n",
    "        ls_result.extend([metrics_fm, metrics_fm_bpr, metrics_mf_bpr])\n",
    "\n",
    "    return ls_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_result = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### douban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_item_features_unique = list(np.unique(df_douban_item_features.iloc[:, 1:].values.flatten()))\n",
    "\n",
    "douban_data = Dataset()\n",
    "douban_data.fit(users=df_douban_inter['user_id'], items=df_douban_inter['item_id'])\n",
    "\n",
    "douban_data.fit_partial(\n",
    "    users=df_douban_user_features['user_id'],\n",
    "    user_features=df_douban_user_features['location']\n",
    ")\n",
    "\n",
    "douban_data.fit_partial(\n",
    "    items=df_douban_item_features['item_id'],\n",
    "    item_features=ls_item_features_unique\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user features shape:  (11699, 12135)\n",
      "item features shape:  (22347, 35034)\n"
     ]
    }
   ],
   "source": [
    "print('user features shape: ', douban_data.user_features_shape())\n",
    "print('item features shape: ', douban_data.item_features_shape())\n",
    "user_features = douban_data.build_user_features(((x['user_id'], [*x[1:]]) for i, x in df_douban_user_features.iterrows()))\n",
    "item_features = douban_data.build_item_features(((x['item_id'], [*x[1:]]) for i, x in df_douban_item_features.iterrows()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Fold 0 *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [00:23<00:00,  1.29it/s]\n",
      "Epoch: 100%|██████████| 30/30 [00:40<00:00,  1.36s/it]\n",
      "Epoch: 100%|██████████| 30/30 [00:15<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Fold 1 *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [00:38<00:00,  1.27s/it]\n",
      "Epoch: 100%|██████████| 30/30 [00:41<00:00,  1.39s/it]\n",
      "Epoch: 100%|██████████| 30/30 [01:16<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Fold 2 *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [00:37<00:00,  1.27s/it]\n",
      "Epoch: 100%|██████████| 30/30 [00:39<00:00,  1.32s/it]\n",
      "Epoch: 100%|██████████| 30/30 [01:08<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Fold 3 *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [00:30<00:00,  1.00s/it]\n",
      "Epoch: 100%|██████████| 30/30 [00:42<00:00,  1.43s/it]\n",
      "Epoch: 100%|██████████| 30/30 [00:51<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Fold 4 *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [00:18<00:00,  1.62it/s]\n",
      "Epoch: 100%|██████████| 30/30 [00:35<00:00,  1.18s/it]\n",
      "Epoch: 100%|██████████| 30/30 [00:06<00:00,  4.88it/s]\n"
     ]
    }
   ],
   "source": [
    "ls_result = calc_all_fm_model(df_douban_inter, douban_data, user_features, item_features, 'douban')\n",
    "all_result.extend(ls_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "with out features:\n",
    "    recall:  0.04407316747683846\n",
    "    precision:  0.11393753\n",
    "\n",
    "with features:\n",
    "    recall:  0.003990892983807165\n",
    "    precision:  0.014177823\n",
    "</pre>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### movie lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_user_features_unique = list(np.unique(df_movie_user_features.iloc[:, 1:].values.flatten()))\n",
    "ls_item_features_unique = list(np.unique(df_movie_item_features.iloc[:, 1:].values.flatten()))\n",
    "\n",
    "movie_data = Dataset()\n",
    "movie_data.fit(users=df_movie_inter['user_id'], items=df_movie_inter['item_id'])\n",
    "\n",
    "movie_data.fit_partial(\n",
    "    users=df_movie_user_features['user_id'],\n",
    "    user_features=ls_user_features_unique\n",
    ")\n",
    "\n",
    "movie_data.fit_partial(\n",
    "    items=df_movie_item_features['item_id'],\n",
    "    item_features=ls_item_features_unique\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user features shape:  (943, 972)\n",
      "item features shape:  (1682, 1718)\n"
     ]
    }
   ],
   "source": [
    "print('user features shape: ', movie_data.user_features_shape())\n",
    "print('item features shape: ', movie_data.item_features_shape())\n",
    "user_features = movie_data.build_user_features(((x['user_id'], [*x[1:]]) for i, x in df_movie_user_features.iterrows()))\n",
    "item_features = movie_data.build_item_features(((x['item_id'], [*x[1:]]) for i, x in df_movie_item_features.iterrows()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Fold 0 *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [00:47<00:00,  1.58s/it]\n",
      "Epoch: 100%|██████████| 30/30 [01:49<00:00,  3.65s/it]\n",
      "Epoch: 100%|██████████| 30/30 [00:07<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Fold 1 *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [00:46<00:00,  1.55s/it]\n",
      "Epoch: 100%|██████████| 30/30 [01:45<00:00,  3.52s/it]\n",
      "Epoch: 100%|██████████| 30/30 [00:03<00:00,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Fold 2 *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [00:46<00:00,  1.56s/it]\n",
      "Epoch: 100%|██████████| 30/30 [01:41<00:00,  3.38s/it]\n",
      "Epoch: 100%|██████████| 30/30 [00:03<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Fold 3 *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [00:45<00:00,  1.51s/it]\n",
      "Epoch: 100%|██████████| 30/30 [01:34<00:00,  3.16s/it]\n",
      "Epoch: 100%|██████████| 30/30 [00:03<00:00,  9.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Fold 4 *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [00:46<00:00,  1.56s/it]\n",
      "Epoch: 100%|██████████| 30/30 [01:42<00:00,  3.41s/it]\n",
      "Epoch: 100%|██████████| 30/30 [00:01<00:00, 29.57it/s]\n"
     ]
    }
   ],
   "source": [
    "ls_result = calc_all_fm_model(df_movie_inter, movie_data, user_features, item_features, 'movie_lens')\n",
    "all_result.extend(ls_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_user_features_unique = list(np.unique(df_yelp_user_features.iloc[:, 1:].values.flatten()))\n",
    "ls_item_features_unique = list(np.unique(df_yelp_item_features.iloc[:, 1:].values.flatten()))\n",
    "\n",
    "yelp_data = Dataset()\n",
    "yelp_data.fit(users=df_yelp_inter['user_id'], items=df_yelp_inter['item_id'])\n",
    "\n",
    "yelp_data.fit_partial(\n",
    "    users=df_yelp_user_features['user_id'],\n",
    "    user_features=ls_user_features_unique\n",
    ")\n",
    "\n",
    "yelp_data.fit_partial(\n",
    "    items=df_yelp_item_features['item_id'],\n",
    "    item_features=ls_item_features_unique\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user features shape:  (8533, 8555)\n",
      "item features shape:  (14284, 15400)\n"
     ]
    }
   ],
   "source": [
    "print('user features shape: ', yelp_data.user_features_shape())\n",
    "print('item features shape: ', yelp_data.item_features_shape())\n",
    "user_features = yelp_data.build_user_features(((x['user_id'], [*x[1:]]) for i, x in df_yelp_user_features.iterrows()))\n",
    "item_features = yelp_data.build_item_features(((x['item_id'], [*x[1:]]) for i, x in df_yelp_item_features.iterrows()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Fold 0 *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [16:19<00:00, 32.64s/it]\n",
      "Epoch: 100%|██████████| 30/30 [35:11<00:00, 70.38s/it]\n",
      "Epoch: 100%|██████████| 30/30 [00:16<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Fold 1 *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [15:58<00:00, 31.93s/it]\n",
      "Epoch: 100%|██████████| 30/30 [34:40<00:00, 69.34s/it]\n",
      "Epoch: 100%|██████████| 30/30 [00:01<00:00, 20.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Fold 2 *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [15:43<00:00, 31.47s/it]\n",
      "Epoch: 100%|██████████| 30/30 [31:59<00:00, 63.98s/it]\n",
      "Epoch: 100%|██████████| 30/30 [00:06<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Fold 3 *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [16:18<00:00, 32.62s/it]\n",
      "Epoch: 100%|██████████| 30/30 [32:37<00:00, 65.24s/it]\n",
      "Epoch: 100%|██████████| 30/30 [00:17<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Fold 4 *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [16:40<00:00, 33.35s/it]\n",
      "Epoch: 100%|██████████| 30/30 [33:57<00:00, 67.92s/it]\n",
      "Epoch: 100%|██████████| 30/30 [00:15<00:00,  1.95it/s]\n"
     ]
    }
   ],
   "source": [
    "ls_result = calc_all_fm_model(df_yelp_inter, yelp_data, user_features, item_features, 'yelp')\n",
    "all_result.extend(ls_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_result, columns=['model', 'kfold', 'data', 'Recall', 'NDCG']).to_csv('output/FM_result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterhub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
